# %%
import numpy as np
import boto3
import os
import datetime
import time
import sys
import json

class Simulation():
    '''
    Run simulation to get the classification result y
    '''
    def __init__(self, x, resourcelimit = 1, **kwargs):
        self.result = None
        self.x = x
        # AWS batch job name and definition
        self.jobqueue = 'SVM_Opt_Test'
        self.jobdefinition = 'SVM_Test_Batch_RefInput'
        self.bucketforinput = 'testbucketjoonjae'
        self.bucketforoutput = 'testbucketjoonjae'
        # data folder to save input in local computer
        folder = "data"        
        self.localfoldername = folder
        self.resourcelimit = resourcelimit # max number of jobs at the same time

        # check whether parallel job is needed
        if len(x.shape) == 1:
            self.arraysize = 1
        else:
            self.arraysize = min(x.shape[0], self.resourcelimit)
        
        # take the time and use it for job id
        d = datetime.datetime.now()        
        self.timestamp = str(d.year) + str(d.month) + str(d.day) + str(d.hour) + str(d.minute) + str(d.second)
        self.jobid = []
        self.bucketoutputpath = []
        self.localoutputpath = []

        self.tot_batch_job = self.x.shape[0] // self.arraysize + (self.x.shape[0] % self.arraysize > 0) # total number of batch jobs 
        residue = self.x.shape[0] % self.arraysize
        self.last_batch_job_arraysize = residue if residue > 0 else self.arraysize # array size of the last batch job

        # check folder in local path
        os.chdir(".")
        # make local folder for input data if it does not exist
        if not os.path.isdir(folder):
            os.mkdir(folder)

        # input text file name generation depending on the number of parallel jobs
        self.inputname = []

        # generate input files using x
        inputindex = 0 
        for num_batch_job in range(self.tot_batch_job):
            if num_batch_job == self.tot_batch_job - 1:
                arraysize = self.last_batch_job_arraysize
            else:
                arraysize = self.arraysize 
            for array in range(arraysize):
                inputname = 'input_batch_' + str(num_batch_job) + '_array_' + str(array) + '.txt'
                self.inputname.append(inputname)
                with open(self.localfoldername + '/' + inputname, 'w') as f:
    #                print('self array size', self.arraysize, 'iteration array', array)
                    f.write(str(np.atleast_2d(x)[inputindex, :]))
                inputindex += 1

    def run(self):
        '''
        Submit AWS batch job using input files generated by __init__
        '''
        # Send input text files to s3 bucket
        for array in range(self.x.shape[0]):
            self.send_data(self.bucketforinput, self.localfoldername + '/' + self.inputname[array], 'input' + '/' + self.inputname[array])

        # submit batch job
        for num_batch_job in range(self.tot_batch_job):
            jobname = 'SVM' + self.timestamp + '_batch_' + str(num_batch_job)
            if num_batch_job == self.tot_batch_job - 1:
                arraysize = self.last_batch_job_arraysize
            else:
                arraysize = self.arraysize

            jobid = self.submit_batch(jobname, self.jobqueue, self.jobdefinition, arraysize = arraysize)   
            print("Job ", jobname, " is submitted")

            # check batch status
            if self.checkbatchstatus(jobid) == True: # if batch job succeeded
                # pick random class for dummy test and upload to bucket for testing
                # should be removed for future use
                ####################################################
                for array in range(arraysize):
                    y_data = np.random.randint(2)
                    data = {}
                    data['status'] = y_data
                    localfilename = 'output_batch_' + str(num_batch_job) + '_array_' + str(array) + '.json'
                    bucketfilename = 'simulation_output/' + 'output_batch_' + str(num_batch_job) + '_array_' + str(array) + '.json'
                    
                    with open(localfilename, 'w') as f:
                        json.dump(data, f)
                    
                    resource = boto3.resource('s3')
                    resource.meta.client.upload_file(localfilename, 'testbucketjoonjae', bucketfilename)
                    self.localoutputpath.append(localfilename)
                    self.bucketoutputpath.append(bucketfilename)
                ######################################################

            else:
                print("Simulation fails, so algorithm is terminated")
                sys.exit()     

        # read true result from s3 bucket
        y_list = self.read_resultfroms3('testbucketjoonjae', )                
        print('y is obtained')

        filter = lambda x: -1 if x == 0 else x
        y_list = list(map(filter, y_list))

        return y_list

    def send_data(self, bucketname, localfilename, cloudfilename = 'input/input.txt'):
        # send text file to server for simulation
        # adequate formating should be known in the future
        resource = boto3.resource('s3')
        resource.meta.client.upload_file(localfilename, bucketname, cloudfilename)
        print('File name ', localfilename, ' is sent to ', bucketname, ' bucket as ', cloudfilename)


    def checkbatchstatus(self, jobid, max_retry = 10):
        client = boto3.client('batch')        

        wait = True
        success = False
        trial = 0

        while wait and trial <= max_retry :
            jobinfo = client.describe_jobs(jobs = [jobid])['jobs'][0]                        
            job_status = jobinfo['status']

            if job_status == 'SUCCEEDED':
                print('Job is done successfully')
                success = True
                break

            elif job_status == 'FAILED':
                break

            else:
                print('Current job status:', job_status)
                time.sleep(60)
                trial += 1

        return success

    def read_resultfroms3(self, bucketname):
        
        y_list = []
        # download data from s3
        s3 = boto3.resource('s3')

        for localpath, bucketpath in zip(self.localoutputpath, self.bucketoutputpath):
            s3.Bucket(bucketname).download_file(bucketpath, localpath)
            with open(localpath, 'r') as f:
                result = json.load(f)

            y_list.append(result['status'])

        print('json file downloaded')

        # read result text file from server
        return y_list

    def postprocessing_result(self, result_text):
        # postprocess text to be used in Python
        processed_result = result_text
        return processed_result 


    def submit_batch(self, jobname, jobqueue, jobdefinition, arraysize = 1):
        client = boto3.client('batch')

        if arraysize == 1 or arraysize == 0 :
            response = client.submit_job(
                jobName = jobname,
                jobQueue = jobqueue,
                jobDefinition = jobdefinition,

    #            containerOverrides = containeroverrides,
    #            parameters = parameters
            )
        else:
            response = client.submit_job(
                jobName = jobname,
                jobQueue = jobqueue,
                jobDefinition = jobdefinition,
                arrayProperties = {'size' : arraysize}
    #            containerOverrides = containeroverrides,
    #            parameters = parameters
            )

        jobID = response['jobId']
        self.jobid.append(jobID)
        return jobID